{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Colecting and analyzing the timing profile of code\n",
    "* The goal of this notebook is to help you with HW3  \n",
    "* This code is available from the class notes on Canvas  \n",
    "* You can use this code for debugging, but when you submit - use the original nbgrader notebook provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Timer: Time Measurement Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     9,
     14,
     18
    ],
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b1c3c0869d88c97b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "import datetime\n",
    "\n",
    "import collections\n",
    "\n",
    "class Timer:\n",
    "    def __init__(self):\n",
    "        self.start()\n",
    "        \n",
    "    def start(self):\n",
    "        \"\"\" Start the timer \"\"\"\n",
    "        self.start = time()\n",
    "        self.timestamps = collections.OrderedDict()\n",
    "        \n",
    "    def record(self, key):\n",
    "        \"\"\" Record a timestamp with the label `key` \"\"\"\n",
    "        self.timestamps[key] = time()\n",
    "        \n",
    "    def print_interval(self):\n",
    "        \"\"\" Print all recorded times in order \"\"\"\n",
    "        key_times = list(self.timestamps.items())\n",
    "        for i in range(len(key_times)):\n",
    "            prev = self.start if i == 0 else key_times[i - 1][1]\n",
    "            \n",
    "            print(f'{key_times[i][0]}: {key_times[i][1] - prev:12.2f} seconds')\n",
    "            \n",
    "timer = Timer()\n",
    "timer.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "_version = '10gb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4b222549daef473e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.rdd import RDD\n",
    "\n",
    "def set_spark_config(leader_name=None, app_name=\"cse255 spark\"):\n",
    "    import os\n",
    "    from pyspark import SparkContext, SparkConf\n",
    "\n",
    "    def get_local_ip():\n",
    "        import socket\n",
    "        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
    "        s.connect((\"8.8.8.8\", 80))\n",
    "        ip = s.getsockname()[0]\n",
    "        s.close()\n",
    "        return ip\n",
    "\n",
    "    if leader_name is not None:\n",
    "        # Connect to the treasurer's spark clusters\n",
    "        os.environ['SPARK_LOCAL_IP'] = \"\" #driver_host\n",
    "        driver_host = get_local_ip()\n",
    "\n",
    "        conf = SparkConf()\n",
    "        conf.setMaster(f'spark://spark-master.{leader_name}.svc.cluster.local:7077')\n",
    "        conf.set(\"spark.blockmanager.port\", \"50002\")\n",
    "        conf.set(\"spark.driver.bindAddress\", driver_host)\n",
    "        conf.set(\"spark.driver.host\", driver_host)\n",
    "        conf.set(\"spark.driver.port\", \"50500\")\n",
    "        conf.set('spark.authenticate', False)\n",
    "    else:\n",
    "        conf = SparkConf()\n",
    "        \n",
    "#     conf.set(\"spark.cores.max\", 4)\n",
    "    conf.set(\"spark.executor.memory\", \"20g\")\n",
    "    conf.setAppName(app_name)\n",
    "    sc = SparkContext(conf=conf)\n",
    "\n",
    "    return sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3f1a2a3a11884244",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Use local clusters (None) while developing in the notebook \n",
    "sc = set_spark_config(leader_name=\"l1qiao\")\n",
    "\n",
    "# Test whether cluster resources are available\n",
    "sc.parallelize(['Test', 'resources']).collect()\n",
    "\n",
    "timer.record(\"set up sc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "file_path = f'file:///datasets/cs255-sp22-a00-public/public/hw2-files-{_version}.txt'\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raw_rdd = sc.textFile(file_path).cache()\n",
    "num_tweets = raw_rdd.count()\n",
    "# YOUR CODE ENDS\n",
    "\n",
    "timer.record(\"read data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     2
    ],
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def safe_parse(raw_json):\n",
    "    \"\"\"\n",
    "    Input is a String\n",
    "    Output is a JSON object if the tweet is valid and None if not valid\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    try:\n",
    "        json_obj = json.loads(raw_json) # Not broken\n",
    "        json_obj[\"created_at\"] # Not a message\n",
    "        return json_obj\n",
    "    except:\n",
    "        return None\n",
    "    # YOUR CODE ENDS\n",
    "    \n",
    "# Remember to construct an RDD of (user_id, text) here.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raw_rdd_json = raw_rdd.map(safe_parse).filter(lambda p: p)\n",
    "user_text_rdd = raw_rdd_json.map(lambda p: (p['user']['id_str'], p['text']))\n",
    "# YOUR CODE ENDS\n",
    "\n",
    "timer.record(\"safe parse rdd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "user_rdd = user_text_rdd.map(lambda user_text: user_text[0])\n",
    "num_unique_users = user_rdd.distinct().count()\n",
    "# YOUR CODE ENDS\n",
    "\n",
    "timer.record(\"count unique users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2dfb9ca4c5d82ccc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "452743"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "import pickle\n",
    "\n",
    "proc = subprocess.Popen([\"cat\", \"./users-partition.pickle\"], stdout=subprocess.PIPE)\n",
    "pickle_content = proc.communicate()[0]\n",
    "\n",
    "partition = pickle.loads(pickle_content)\n",
    "## Using Broadcast would help you here\n",
    "\n",
    "len(partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "from operator import add\n",
    "partition_count_rdd = user_text_rdd.map(lambda user_text: (partition.get(user_text[0], 7), 1)).reduceByKey(add).sortByKey()\n",
    "counts_per_part = partition_count_rdd.collect()\n",
    "# YOUR CODE ENDS\n",
    "\n",
    "timer.record(\"count tweets per user partition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ],
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cafbc0f730707701",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "\"\"\"\n",
    "This code implements a basic, Twitter-aware tokenizer.\n",
    "\n",
    "A tokenizer is a function that splits a string of text into words. In\n",
    "Python terms, we map string and unicode objects into lists of unicode\n",
    "objects.\n",
    "\n",
    "There is not a single right way to do tokenizing. The best method\n",
    "depends on the application.  This tokenizer is designed to be flexible\n",
    "and this easy to adapt to new domains and tasks.  The basic logic is\n",
    "this:\n",
    "\n",
    "1. The tuple regex_strings defines a list of regular expression\n",
    "   strings.\n",
    "\n",
    "2. The regex_strings strings are put, in order, into a compiled\n",
    "   regular expression object called word_re.\n",
    "\n",
    "3. The tokenization is done by word_re.findall(s), where s is the\n",
    "   user-supplied string, inside the tokenize() method of the class\n",
    "   Tokenizer.\n",
    "\n",
    "4. When instantiating Tokenizer objects, there is a single option:\n",
    "   preserve_case.  By default, it is set to True. If it is set to\n",
    "   False, then the tokenizer will downcase everything except for\n",
    "   emoticons.\n",
    "\n",
    "The __main__ method illustrates by tokenizing a few examples.\n",
    "\n",
    "I've also included a Tokenizer method tokenize_random_tweet(). If the\n",
    "twitter library is installed (http://code.google.com/p/python-twitter/)\n",
    "and Twitter is cooperating, then it should tokenize a random\n",
    "English-language tweet.\n",
    "\n",
    "\n",
    "Julaiti Alafate:\n",
    "  I modified the regex strings to extract URLs in tweets.\n",
    "\"\"\"\n",
    "\n",
    "__author__ = \"Christopher Potts\"\n",
    "__copyright__ = \"Copyright 2011, Christopher Potts\"\n",
    "__credits__ = []\n",
    "__license__ = \"Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License: http://creativecommons.org/licenses/by-nc-sa/3.0/\"\n",
    "__version__ = \"1.0\"\n",
    "__maintainer__ = \"Christopher Potts\"\n",
    "__email__ = \"See the author's website\"\n",
    "\n",
    "######################################################################\n",
    "\n",
    "import re\n",
    "from html import entities \n",
    "\n",
    "######################################################################\n",
    "# The following strings are components in the regular expression\n",
    "# that is used for tokenizing. It's important that phone_number\n",
    "# appears first in the final regex (since it can contain whitespace).\n",
    "# It also could matter that tags comes after emoticons, due to the\n",
    "# possibility of having text like\n",
    "#\n",
    "#     <:| and some text >:)\n",
    "#\n",
    "# Most imporatantly, the final element should always be last, since it\n",
    "# does a last ditch whitespace-based tokenization of whatever is left.\n",
    "\n",
    "# This particular element is used in a couple ways, so we define it\n",
    "# with a name:\n",
    "emoticon_string = r\"\"\"\n",
    "    (?:\n",
    "      [<>]?\n",
    "      [:;=8]                     # eyes\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth      \n",
    "      |\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [:;=8]                     # eyes\n",
    "      [<>]?\n",
    "    )\"\"\"\n",
    "\n",
    "# The components of the tokenizer:\n",
    "regex_strings = (\n",
    "    # Phone numbers:\n",
    "    r\"\"\"\n",
    "    (?:\n",
    "      (?:            # (international)\n",
    "        \\+?[01]\n",
    "        [\\-\\s.]*\n",
    "      )?            \n",
    "      (?:            # (area code)\n",
    "        [\\(]?\n",
    "        \\d{3}\n",
    "        [\\-\\s.\\)]*\n",
    "      )?    \n",
    "      \\d{3}          # exchange\n",
    "      [\\-\\s.]*   \n",
    "      \\d{4}          # base\n",
    "    )\"\"\"\n",
    "    ,\n",
    "    # URLs:\n",
    "    r\"\"\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"\"\"\n",
    "    ,\n",
    "    # Emoticons:\n",
    "    emoticon_string\n",
    "    ,    \n",
    "    # HTML tags:\n",
    "     r\"\"\"<[^>]+>\"\"\"\n",
    "    ,\n",
    "    # Twitter username:\n",
    "    r\"\"\"(?:@[\\w_]+)\"\"\"\n",
    "    ,\n",
    "    # Twitter hashtags:\n",
    "    r\"\"\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\"\"\"\n",
    "    ,\n",
    "    # Remaining word types:\n",
    "    r\"\"\"\n",
    "    (?:[a-z][a-z'\\-_]+[a-z])       # Words with apostrophes or dashes.\n",
    "    |\n",
    "    (?:[+\\-]?\\d+[,/.:-]\\d+[+\\-]?)  # Numbers, including fractions, decimals.\n",
    "    |\n",
    "    (?:[\\w_]+)                     # Words without apostrophes or dashes.\n",
    "    |\n",
    "    (?:\\.(?:\\s*\\.){1,})            # Ellipsis dots. \n",
    "    |\n",
    "    (?:\\S)                         # Everything else that isn't whitespace.\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "######################################################################\n",
    "# This is the core tokenizing regex:\n",
    "    \n",
    "word_re = re.compile(r\"\"\"(%s)\"\"\" % \"|\".join(regex_strings), re.VERBOSE | re.I | re.UNICODE)\n",
    "\n",
    "# The emoticon string gets its own regex so that we can preserve case for them as needed:\n",
    "emoticon_re = re.compile(regex_strings[1], re.VERBOSE | re.I | re.UNICODE)\n",
    "\n",
    "# These are for regularizing HTML entities to Unicode:\n",
    "html_entity_digit_re = re.compile(r\"&#\\d+;\")\n",
    "html_entity_alpha_re = re.compile(r\"&\\w+;\")\n",
    "amp = \"&amp;\"\n",
    "\n",
    "######################################################################\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, preserve_case=False):\n",
    "        self.preserve_case = preserve_case\n",
    "\n",
    "    def tokenize(self, s):\n",
    "        \"\"\"\n",
    "        Argument: s -- any string or unicode object\n",
    "        Value: a tokenize list of strings; conatenating this list returns the original string if preserve_case=False\n",
    "        \"\"\"        \n",
    "        # Try to ensure unicode:\n",
    "        try:\n",
    "            s = str(s)\n",
    "        except UnicodeDecodeError:\n",
    "            s = s.encode('string_escape')\n",
    "            s = str(s)\n",
    "        # Fix HTML character entitites:\n",
    "        s = self.__html2unicode(s)\n",
    "        # Tokenize:\n",
    "        words = word_re.findall(s)\n",
    "        # Possible alter the case, but avoid changing emoticons like :D into :d:\n",
    "        if not self.preserve_case:            \n",
    "            words = list(map((lambda x : x if emoticon_re.search(x) else x.lower()), words))\n",
    "        return words\n",
    "\n",
    "    def tokenize_random_tweet(self):\n",
    "        \"\"\"\n",
    "        If the twitter library is installed and a twitter connection\n",
    "        can be established, then tokenize a random tweet.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            import twitter\n",
    "        except ImportError:\n",
    "            print(\"Apologies. The random tweet functionality requires the Python twitter library: http://code.google.com/p/python-twitter/\")\n",
    "        from random import shuffle\n",
    "        api = twitter.Api()\n",
    "        tweets = api.GetPublicTimeline()\n",
    "        if tweets:\n",
    "            for tweet in tweets:\n",
    "                if tweet.user.lang == 'en':            \n",
    "                    return self.tokenize(tweet.text)\n",
    "        else:\n",
    "            raise Exception(\"Apologies. I couldn't get Twitter to give me a public English-language tweet. Perhaps try again\")\n",
    "\n",
    "    def __html2unicode(self, s):\n",
    "        \"\"\"\n",
    "        Internal metod that seeks to replace all the HTML entities in\n",
    "        s with their corresponding unicode characters.\n",
    "        \"\"\"\n",
    "        # First the digits:\n",
    "        ents = set(html_entity_digit_re.findall(s))\n",
    "        if len(ents) > 0:\n",
    "            for ent in ents:\n",
    "                entnum = ent[2:-1]\n",
    "                try:\n",
    "                    entnum = int(entnum)\n",
    "                    s = s.replace(ent, unichr(entnum))\t\n",
    "                except:\n",
    "                    pass\n",
    "        # Now the alpha versions:\n",
    "        ents = set(html_entity_alpha_re.findall(s))\n",
    "        ents = filter((lambda x : x != amp), ents)\n",
    "        for ent in ents:\n",
    "            entname = ent[1:-1]\n",
    "            try:            \n",
    "                s = s.replace(ent, unichr(entities.name2codepoint[entname]))\n",
    "            except:\n",
    "                pass                    \n",
    "            s = s.replace(amp, \" and \")\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9cc0f18f3116753b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "tok = Tokenizer(preserve_case=False)\n",
    "\n",
    "def get_rel_popularity(c_k, c_all):\n",
    "    '''\n",
    "    Compute the relative popularity of a token.\n",
    "    \n",
    "    Args:\n",
    "        c_k: the number of mentions in the user partition k.\n",
    "        c_all: the number of all mentions.\n",
    "        \n",
    "    Return:\n",
    "        The relative popularity of the token. It should be a negative number due to the log function. \n",
    "    '''\n",
    "    \n",
    "    return log(1.0 * c_k / c_all) / log(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "user_token_rdd = user_text_rdd \\\n",
    "    .map(lambda x: (x[0], set(tok.tokenize(x[1])))) \\\n",
    "    .groupByKey() \\\n",
    "    .flatMap(lambda x: [(x[0], token) for token in set.union(*x[1])])\n",
    "token_count_rdd = user_token_rdd.map(lambda user_token: (user_token[1], 1)).reduceByKey(add) \n",
    "num_of_tokens = token_count_rdd.count()\n",
    "# YOUR CODE ENDS\n",
    "\n",
    "timer.record(\"count all unique tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "user_token_rdd_freq = token_count_rdd.filter(lambda tok_count: tok_count[1] >= 100)\n",
    "num_freq_tokens = user_token_rdd_freq.count()\n",
    "top20 = user_token_rdd_freq.sortBy(lambda tok_count: tok_count[1], ascending=False).take(20)\n",
    "# YOUR CODE ENDS\n",
    "\n",
    "timer.record(\"count overall most popular tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     16,
     22
    ],
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# user_token_rdd_freq: [(token, count), ...] where count >= 100\n",
    "user_token_dict_freq = user_token_rdd_freq.collectAsMap()\n",
    "timer.record(f'user_token_dict_freq')\n",
    "\n",
    "popular_10_in_each_group = []\n",
    "for k in range(8):\n",
    "    # user_token_rdd: [(token, count), ...]\n",
    "    # k_group_token: [(user, token), ...] where the count of token >= 100 AND user is in group k\n",
    "    \n",
    "    k_group_token = user_token_rdd \\ \n",
    "        .filter(lambda user_token: user_token[1] in user_token_dict_freq) \\\n",
    "        .filter(lambda user_token: partition.get(user_token[0], 7) == k)\n",
    "    timer.record(f'iteration {k} k_group_token')\n",
    "\n",
    "    # k_group_token_count: [(token, count), ...] where count is number of mentions of token in group k\n",
    "    k_group_token_count = k_group_token \\\n",
    "        .map(lambda user_token: (user_token[1], 1)) \\\n",
    "        .reduceByKey(lambda x, y: x + y) \n",
    "    timer.record(f'iteration {k} k_group_token_count')\n",
    "    \n",
    "    # Get the all mentions of the token regardless of the group using user_token_dict_freq[token]\n",
    "    top10 = k_group_token_count \\\n",
    "        .map(lambda token_count: (token_count[0], get_rel_popularity(token_count[1], user_token_dict_freq[token_count[0]]))) \\\n",
    "        .sortBy(lambda token_rel: (-token_rel[1], token_rel[0])) \\\n",
    "        .take(10)\n",
    "    timer.record(f'end iteration {k} top10')\n",
    "                \n",
    "    popular_10_in_each_group.append(top10)\n",
    "# YOUR CODE END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set up sc:         6.32 seconds\n",
      "read data:        51.98 seconds\n",
      "safe parse rdd:         0.01 seconds\n",
      "count unique users:        89.39 seconds\n",
      "count tweets per user partition:       100.91 seconds\n",
      "count all unique tokens:       140.34 seconds\n",
      "count overall most popular tokens:        45.07 seconds\n",
      "user_token_dict_freq:        11.12 seconds\n",
      "iteration 0 k_group_token:         0.00 seconds\n",
      "iteration 0 k_group_token_count:         4.25 seconds\n",
      "end iteration 0 top10:        58.54 seconds\n",
      "iteration 1 k_group_token:         0.00 seconds\n",
      "iteration 1 k_group_token_count:         4.52 seconds\n",
      "end iteration 1 top10:        62.99 seconds\n",
      "iteration 2 k_group_token:         0.00 seconds\n",
      "iteration 2 k_group_token_count:         4.55 seconds\n",
      "end iteration 2 top10:        59.49 seconds\n",
      "iteration 3 k_group_token:         0.00 seconds\n",
      "iteration 3 k_group_token_count:         4.50 seconds\n",
      "end iteration 3 top10:        63.93 seconds\n",
      "iteration 4 k_group_token:         0.00 seconds\n",
      "iteration 4 k_group_token_count:         3.91 seconds\n",
      "end iteration 4 top10:        63.20 seconds\n",
      "iteration 5 k_group_token:         0.00 seconds\n",
      "iteration 5 k_group_token_count:         4.29 seconds\n",
      "end iteration 5 top10:        64.53 seconds\n",
      "iteration 6 k_group_token:         0.00 seconds\n",
      "iteration 6 k_group_token_count:         3.95 seconds\n",
      "end iteration 6 top10:        64.27 seconds\n",
      "iteration 7 k_group_token:         0.00 seconds\n",
      "iteration 7 k_group_token_count:         4.01 seconds\n",
      "end iteration 7 top10:        68.11 seconds\n"
     ]
    }
   ],
   "source": [
    "timer.print_interval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Takeaways\n",
    "* Use a timer to understand which parts of your code consume the most time.\n",
    "* Using a sequential timer rather than `start` `stop` guarantees that all parts are covered.\n",
    "* When the same RDD is computed over and over, add a `.cache()` to store the the ressults.\n",
    "* Avoid Loops: an ideal code does not have loops.\n",
    "* Running time varies by $\\pm 5\\%$ even when your process runs alone. Code will be run only once in evaluation, so it might be better or worse than the timing you got. To estimate the range of running times, run your code several times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

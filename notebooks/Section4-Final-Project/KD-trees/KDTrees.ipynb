{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e236584e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n",
      "/Users/yoavfreund/datasets/poverty_v1.1/partitioned_images/rural/kenya/\n",
      "/Users/yoavfreund/datasets/poverty_v1.1/partitioned_images/rural/benin/\n",
      "/Users/yoavfreund/datasets/poverty_v1.1/partitioned_images/rural/tanzania/\n",
      "/Users/yoavfreund/datasets/poverty_v1.1/partitioned_images/rural/zambia/\n",
      "/Users/yoavfreund/datasets/poverty_v1.1/partitioned_images/rural/ghana/\n",
      "/Users/yoavfreund/datasets/poverty_v1.1/partitioned_images/rural/malawi/\n",
      "/Users/yoavfreund/datasets/poverty_v1.1/partitioned_images/rural/zimbabwe/\n",
      "/Users/yoavfreund/datasets/poverty_v1.1/partitioned_images/rural/democratic_republic_of_congo/\n",
      "/Users/yoavfreund/datasets/poverty_v1.1/partitioned_images/rural/lesotho/\n",
      "/Users/yoavfreund/datasets/poverty_v1.1/partitioned_images/rural/mali/\n",
      "/Users/yoavfreund/datasets/poverty_v1.1/partitioned_images/rural/nigeria/\n",
      "/Users/yoavfreund/datasets/poverty_v1.1/partitioned_images/rural/burkina_faso/\n",
      "/Users/yoavfreund/datasets/poverty_v1.1/partitioned_images/rural/mozambique/\n",
      "/Users/yoavfreund/datasets/poverty_v1.1/partitioned_images/rural/angola/\n",
      "/Users/yoavfreund/datasets/poverty_v1.1/partitioned_images/rural/senegal/\n",
      "/Users/yoavfreund/datasets/poverty_v1.1/partitioned_images/rural/ethiopia/\n",
      "/Users/yoavfreund/datasets/poverty_v1.1/partitioned_images/rural/togo/\n",
      "/Users/yoavfreund/datasets/poverty_v1.1/partitioned_images/rural/uganda/\n",
      "/Users/yoavfreund/datasets/poverty_v1.1/partitioned_images/rural/guinea/\n",
      "/Users/yoavfreund/datasets/poverty_v1.1/partitioned_images/rural/cameroon/\n",
      "/Users/yoavfreund/datasets/poverty_v1.1/partitioned_images/rural/sierra_leone/\n",
      "/Users/yoavfreund/datasets/poverty_v1.1/partitioned_images/rural/rwanda/\n",
      "/Users/yoavfreund/datasets/poverty_v1.1/partitioned_images/rural/cote_d_ivoire/\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "from numpy import load\n",
    "from KDTreeEncoding import *\n",
    "\n",
    "_dir='/Users/yoavfreund/datasets/poverty_v1.1/partitioned_images/'\n",
    "_subdir='rural/*/'\n",
    "\n",
    "from glob import glob\n",
    "_ssdir=glob(f'{_dir}{_subdir}')\n",
    "for _d in _ssdir:\n",
    "    print(_d)\n",
    "    files=glob(_d+'/*.npz')\n",
    "    _len=len(files)\n",
    "    if _len>0:\n",
    "        print(_d,_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ac1b6c7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/yoavfreund/datasets/poverty_v1.1/partitioned_images/rural/kenya',\n",
       " '/Users/yoavfreund/datasets/poverty_v1.1/partitioned_images/rural/benin',\n",
       " '/Users/yoavfreund/datasets/poverty_v1.1/partitioned_images/rural/tanzania',\n",
       " '/Users/yoavfreund/datasets/poverty_v1.1/partitioned_images/rural/zambia',\n",
       " '/Users/yoavfreund/datasets/poverty_v1.1/partitioned_images/rural/ghana',\n",
       " '/Users/yoavfreund/datasets/poverty_v1.1/partitioned_images/rural/malawi',\n",
       " '/Users/yoavfreund/datasets/poverty_v1.1/partitioned_images/rural/zimbabwe',\n",
       " '/Users/yoavfreund/datasets/poverty_v1.1/partitioned_images/rural/democratic_republic_of_congo',\n",
       " '/Users/yoavfreund/datasets/poverty_v1.1/partitioned_images/rural/lesotho',\n",
       " '/Users/yoavfreund/datasets/poverty_v1.1/partitioned_images/rural/mali',\n",
       " '/Users/yoavfreund/datasets/poverty_v1.1/partitioned_images/rural/nigeria',\n",
       " '/Users/yoavfreund/datasets/poverty_v1.1/partitioned_images/rural/burkina_faso',\n",
       " '/Users/yoavfreund/datasets/poverty_v1.1/partitioned_images/rural/mozambique',\n",
       " '/Users/yoavfreund/datasets/poverty_v1.1/partitioned_images/rural/angola',\n",
       " '/Users/yoavfreund/datasets/poverty_v1.1/partitioned_images/rural/senegal',\n",
       " '/Users/yoavfreund/datasets/poverty_v1.1/partitioned_images/rural/ethiopia',\n",
       " '/Users/yoavfreund/datasets/poverty_v1.1/partitioned_images/rural/togo',\n",
       " '/Users/yoavfreund/datasets/poverty_v1.1/partitioned_images/rural/uganda',\n",
       " '/Users/yoavfreund/datasets/poverty_v1.1/partitioned_images/rural/guinea',\n",
       " '/Users/yoavfreund/datasets/poverty_v1.1/partitioned_images/rural/cameroon',\n",
       " '/Users/yoavfreund/datasets/poverty_v1.1/partitioned_images/rural/sierra_leone',\n",
       " '/Users/yoavfreund/datasets/poverty_v1.1/partitioned_images/rural/rwanda',\n",
       " '/Users/yoavfreund/datasets/poverty_v1.1/partitioned_images/rural/cote_d_ivoire']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e92fae75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile KDTreeEncoding.py\n",
    "from numpy import *\n",
    "class KD_tree:\n",
    "    \"\"\"A class that represents the whole KDtree,\n",
    "    Points to the root KD_node\"\"\\\n",
    "    def __init__(self,data,limit=100,depth=8):\n",
    "        \"\"\" Instantiate a KDtree:\n",
    "        data = training data each row is an example, the number of columns is the dimension.\n",
    "        limit,depth  = nodes are split into two children only if their depth is smaller than depth \n",
    "                       and the number of examples in the node is at least limit\"\"\"\n",
    "        self.data_size=data.shape[0]\n",
    "        self.root=KD_node(self,data,limit=limit,depth=depth,path=[])\n",
    "    def calc_encoding(self,data):\n",
    "        \"\"\"calculate a log ratio encoding for a new set of vectors (=image)\"\"\"\n",
    "        data_size=data.shape[0]\n",
    "        return self.root.calc_encoding(data,data_size)\n",
    "\n",
    "class KD_node:\n",
    "    \"\"\" the main class in the implementation of KD-tree, encodes a single node in the tree\"\"\"\n",
    "    def __init__(self,tree,data,limit=100,depth=8,path=[]):\n",
    "        #print(len(path))\n",
    "        self.tree=tree\n",
    "        self.path=path\n",
    "        self.read_path=''.join([str(x) for x in path])\n",
    "        self.size,self.dim=data.shape\n",
    "        self.prob=data.shape[0]/self.tree.data_size\n",
    "        #print('%10s  %3.3g'%(self.read_path,self.prob))\n",
    " \n",
    "        if self.size<limit or len(path)>depth:\n",
    "            self.leaf=True\n",
    "        else:\n",
    "            self.leaf=False\n",
    "            index=random.choice(self.dim)\n",
    "            H=data[:,index]\n",
    "            threshold=median(H)\n",
    "            below=data[data[:,index]<threshold,:]\n",
    "            above=data[data[:,index]>=threshold,:]\n",
    "            self.threshold=threshold\n",
    "            self.index=index\n",
    "            self.above=KD_node(tree,above,path=self.path+[1],depth=depth)\n",
    "            self.below=KD_node(tree,below,path=self.path+[0],depth=depth)\n",
    "\n",
    "    def calc_encoding(self,data,full_data_size,limit=100,smooth=1e-7):\n",
    "        \"\"\"Use trained tree to encode an individual dataset (image)\"\"\"\n",
    "        my_prob=data.shape[0]/full_data_size\n",
    "        log_ratio=log((my_prob+smooth)/(self.prob+smooth))\n",
    "        my_result=[(self.read_path,log_ratio)]\n",
    "        if self.leaf or data.shape[0] < limit:\n",
    "            return my_result\n",
    "        else:\n",
    "            below=data[data[:,self.index]<self.threshold,:]\n",
    "            above=data[data[:,self.index]>=self.threshold,:]\n",
    "            above_results=self.above.calc_encoding(above,full_data_size,limit=limit)\n",
    "            below_results=self.below.calc_encoding(below,full_data_size,limit=limit)\n",
    "            return my_result+above_results+below_results\n",
    "\n",
    "    def calc_density(self,data):\n",
    "        \"\"\"Calculate density in box defined by node\"\"\"\n",
    "        if(data.shape[0]<2):\n",
    "            self.density=0\n",
    "            return 0\n",
    "        bounding_box={i:(min(data[:,i]),max(data[:,i])) for i in range(self.dim)}\n",
    "        Vol=1\n",
    "        for i in range(self.dim):\n",
    "            _min,_max=bounding_box[i]\n",
    "            Vol*=(_max-_min)\n",
    "        self.density=data.shape[0]/(Vol+0.001)\n",
    "        return self.density\n",
    "    \n",
    "def train_encoder(files,max_images=200):\n",
    "    \"\"\"Train an encoding tree using a set of images\n",
    "    If there are more than man_images image, choose max_images from them \n",
    "    by selecting at random w/o replacement\"\"\"\n",
    "    ## Collect data for training\n",
    "    _len=len(files)\n",
    "    if _len<=max_images:    # if more than max_images files, sample max_images without replacement\n",
    "        selected_files=files\n",
    "    else:\n",
    "        I = choice(range(_len),max_images,replace=False)\n",
    "        selected_files=[files[i] for i in I]\n",
    "        print(len(selected_files))\n",
    "\n",
    "    for  i in range(len(selected_files)):\n",
    "        M=np.load(selected_files[i])\n",
    "        Image=M['x']\n",
    "        pixels=Image.reshape((8, -1)).T\n",
    "        Plist.append(pixels)\n",
    "    data=np.concatenate(Plist,axis=0)\n",
    "\n",
    "    ## train tree\n",
    "    train_size=data.shape[0]\n",
    "    tree=KD_tree(data,depth=10)\n",
    "    return train_size,tree\n",
    "\n",
    "def encode_image(file):\n",
    "    M=np.load(file)\n",
    "    Image=M['x']\n",
    "    pixels=Image.reshape((8, -1)).T\n",
    "    code=tree.calc_encoding(pixels)\n",
    "    return code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0f65f3cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "<class '__main__.KD_tree'>\n",
      "CPU times: user 23 s, sys: 4.09 s, total: 27.1 s\n",
      "Wall time: 28.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_size,tree=train_encoder(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "93362f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 yoavfreund  wheel   366K May 14 15:16 tree.pkl\r\n"
     ]
    }
   ],
   "source": [
    "import pickle as pk\n",
    "pk.dump(tree,open('tree.pkl','wb'))\n",
    "!ls -lh tree.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7293d310",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968bc2e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
